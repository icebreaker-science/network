{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "base_path = '/home/jupyter-cchen'\n",
    "private_data_path = os.path.join(base_path, 'data/_private/chaoran')\n",
    "\n",
    "# Define source of entries\n",
    "# number_entries = 7836565\n",
    "number_entries = 3444311\n",
    "# number_entries = 9958\n",
    "def get_entries():\n",
    "    # return data_models.basic_read_from_xz(os.path.join(base_path, 'data/11-basic/basics.json.xz'))\n",
    "    return data_models.read_basics_from_database('localhost', '12210', 'icebreaker_network', 'postgres', 'postgres')\n",
    "\n",
    "# Add path to /src to sys.path\n",
    "script_base_path = os.path.join(base_path, 'src')\n",
    "if script_base_path not in sys.path:\n",
    "  sys.path.append(script_base_path)\n",
    "\n",
    "# Import own modules\n",
    "from data import data_models\n",
    "\n",
    "# Reload own modules (since they will be changing quite often)\n",
    "import importlib\n",
    "importlib.reload(data_models)\n",
    "\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicDataEntry(icebreaker_id=1673331, doi=None, core_id='102549444', title='Concentration-Dependent Profiles for Describing the Scatter of Results of Interlaboratory Surveys', abstract='Summary: In an interlaboratory survey for the quantitative determination of a clinical chemical quantity, samples the same specimen are analysed in different laboratories. If the number of participating laboratories is sufficiently large, then the differences between the 50th percentile (median) and e. g. the 25th and 75th percentiles of the results give a very reliable impression of the ränge of interlaboratory scatter for the particular analytical technique. Results from a relatively large number of interlaboratory surveys, in which specimens containing different concen-trations of the analyte are investigated, can be handled in the same way. If the resulting differences between the ichosen percentiles are plotted against the median, and the corresponding two regression lines (upper and lower) are idrawn, the results are asymmetric scatter profiles covering the concentration ränge of the specimen collective. Numerous options are available. Thus, a profile&apos;s power of characterizing the scatter correctly can be improved by weighting of the results. Moreover, scatter profiles can be based on different variables of the survey, such äs the analytical method, or the observation period, etc. They may be based on the total collective of all results for a given quantity, or they can be constructed for subcollectives of results obtained with a single analytical method. Further, it is possible to present the results of all subcollectives in a single pair of scatter profiles. This latter type of analysis provides profiles of the average scatter for a collective of different analytical methods, which are unaffected by any systematic differences that may exist between the methods', has_full_text=False, year=1992, topics=[], subjects=['text'], language_detected_most_likely='en', language_detected_probabilities=[['en', 0.9999978300987551]])\n"
     ]
    }
   ],
   "source": [
    "for e in get_entries():\n",
    "    print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(private_data_path, 'playground.pickle'), 'wb') as f:\n",
    "    data = {\n",
    "        'topic_counter': topic_counter,\n",
    "        'subject_counter': subject_counter,\n",
    "        'edge_dict': edge_dict,\n",
    "        'G': G,\n",
    "        'G2': G2,\n",
    "        'nodes_label_to_numeric_id': nodes_label_to_numeric_id,\n",
    "        'nodes_numeric_id_to_label': nodes_numeric_id_to_label\n",
    "    }\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(private_data_path, 'playground.pickle'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#     topic_counter = data['topic_counter']\n",
    "#     subject_counter = data['subject_counter']\n",
    "#     has_t = data['stats']['has_t']\n",
    "#     has_s = data['stats']['has_s']\n",
    "#     has_full_text = data['stats']['has_full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = data['G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def count_topics_and_subjects(entries):\n",
    "    topic_counter = Counter()\n",
    "    subject_counter = Counter()\n",
    "\n",
    "    has_t = 0\n",
    "    has_s = 0\n",
    "    has_full_text = 0\n",
    "    reg = re.compile('[a-zA-Z0-9 ]*')\n",
    "    \n",
    "    for i, entry in tqdm(enumerate(entries), total=number_entries): \n",
    "        ts = entry.topics\n",
    "        ss = entry.subjects\n",
    "        if ts is not None and len(ts) > 0:\n",
    "            topic_counter.update([t.lower() for t in ts if reg.fullmatch(t) is not None])\n",
    "            has_t += 1\n",
    "        if ss is not None and len(ss) > 0:\n",
    "            subject_counter.update([s.lower() for s in ss if reg.fullmatch(s) is not None])\n",
    "            has_s += 1\n",
    "        if entry.has_full_text:\n",
    "            has_full_text += 1\n",
    "    print('total entries={}, has topics={}, has subjects={}, has full text={}'.format(i, has_t, has_s, has_full_text))\n",
    "    return topic_counter, subject_counter\n",
    "\n",
    "topic_counter, subject_counter = count_topics_and_subjects(get_entries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topic_counter))\n",
    "print('-----')\n",
    "\n",
    "import csv\n",
    "import io\n",
    "output = io.StringIO()\n",
    "writer = csv.writer(output)\n",
    "pos = 200\n",
    "for x in topic_counter.most_common()[pos:pos+10]:\n",
    "    writer.writerow(x)\n",
    "print(output.getvalue())\n",
    "# Selected: 0, 200, 750, 3000, 10000, 25000 (+50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_entries_to_csv(entries, path):\n",
    "    \"\"\"\n",
    "    Writes a small part of the data as csv to be imported by a relational database\n",
    "    \"\"\"\n",
    "    with open(path, 'wt') as f:\n",
    "        csv_writer = csv.DictWriter(f, fieldnames=['icebreaker_id', 'doi', 'core_id', 'title', 'year'])\n",
    "        csv_writer.writeheader()\n",
    "        for entry in tqdm(entries, total=number_entries):\n",
    "            csv_writer.writerow({\n",
    "                'icebreaker_id': entry.icebreaker_id,\n",
    "                'doi': entry.doi,\n",
    "                'core_id': entry.core_id,\n",
    "                'title': entry.title,\n",
    "                'year': entry.year\n",
    "            })\n",
    "        \n",
    "# write_entries_to_csv(get_entries(), os.path.join(private_data_path, 'basics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def count_edges(entries):\n",
    "    \"\"\"\n",
    "    Computes the weights for all edges. This contains ALL topics - without any filtering!\n",
    "    \"\"\"\n",
    "    edge_dict = {}  # (node1:str, node2:str) -> weight where node1<node2\n",
    "    for entry in tqdm(entries, total=number_entries):\n",
    "        if entry.topics is None:\n",
    "            continue\n",
    "        for i in range(0, len(entry.topics) - 1):\n",
    "            for j in range(i, len(entry.topics)):\n",
    "                t1 = entry.topics[i]\n",
    "                t2 = entry.topics[j]\n",
    "                if t2 < t1:\n",
    "                    tmp = t1\n",
    "                    t1 = t2\n",
    "                    t2 = tmp\n",
    "                if (t1, t2) not in edge_dict:\n",
    "                    edge_dict[(t1, t2)] = 0\n",
    "                edge_dict[(t1, t2)] += 1\n",
    "    return edge_dict\n",
    "\n",
    "edge_dict = count_edges(get_entries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Number of edges: {:d}'.format(len(edge_dict)))\n",
    "\n",
    "df = pd.DataFrame(list(edge_dict.items()), columns = ['edge_name', 'weight']) \n",
    "df.weight.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Actual And Filtered Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def create_network(node_dict, edge_dict, node_set):\n",
    "    \"\"\"\n",
    "    :param node_dict: dict<node id, weight>\n",
    "    :param edge_dict: dict<(node1 id, node2 id), weight>\n",
    "    :param node_set: a set of nodes that should be included in the network; other nodes will be ignored.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for ((n1, n2), w) in tqdm(edge_dict.items(), total=len(edge_dict), desc='Filter edges'):\n",
    "#         if w < 3:\n",
    "#             continue\n",
    "        if n1 == n2:\n",
    "            continue\n",
    "        if n1 not in node_set or n2 not in node_set:\n",
    "            continue\n",
    "        G.add_edge(n1, n2, weight=w)\n",
    "    nx.set_node_attributes(G, topic_counter, name='weight')\n",
    "\n",
    "    # Compute a normalized edge weight\n",
    "    normalized_weights = {}\n",
    "    for e in tqdm(G.edges(data=True), total=len(G.edges()), desc='Computing normalized edge weights'):\n",
    "        n1, n2, data = e\n",
    "        w = data['weight']\n",
    "        normalized_weight = (w / G.degree(n1, weight='weight')) * (w / G.degree(n2, weight='weight'))\n",
    "        normalized_weights[(n1, n2)] = normalized_weight\n",
    "    nx.set_edge_attributes(G, normalized_weights, name='normalized_weight')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def reduce_network(G, k):\n",
    "    \"\"\"\n",
    "    Let's only keep the top k edges per node\n",
    "    \"\"\"\n",
    "    G2 = nx.Graph()\n",
    "    for n1 in tqdm(G.nodes(), total=len(G.nodes()), desc='Reducing network'):\n",
    "        G2.add_node(n1, weight=G.nodes[n1]['weight'])\n",
    "        neighbors = list(G[n1].items())\n",
    "        top_neighbors = sorted(neighbors, key=lambda neigh: neigh[1]['weight'], reverse=True)[:k]\n",
    "        for neigh in top_neighbors:\n",
    "            n2 = neigh[0]\n",
    "            w = neigh[1]['weight']\n",
    "            normalized_w = neigh[1]['normalized_weight']\n",
    "            references = None\n",
    "            if 'references' in neigh[1]:\n",
    "                references = neigh[1]['references']\n",
    "            if n2 < n1:\n",
    "                continue\n",
    "            G2.add_edge(n1, n2, weight=w, normalized_weight=normalized_w, references=references)\n",
    "    return G2\n",
    "\n",
    "def add_references_to_edges(entries, G):\n",
    "    \"\"\"\n",
    "    This function add the corresponding references to all edges\n",
    "    \"\"\"\n",
    "    node_set = set(G.nodes())\n",
    "    edge_dict = {}  # (node1:str, node2:str) -> ids of references as comma separated list where node1<node2\n",
    "    for entry in tqdm(entries, total=number_entries, desc='Add edge references'):\n",
    "        if entry.topics is None:\n",
    "            continue\n",
    "        nodes = list(node_set.intersection(set(entry.topics)))\n",
    "        for i in range(0, len(nodes) - 1):\n",
    "            for j in range(i + 1, len(nodes)):\n",
    "                t1 = nodes[i]\n",
    "                t2 = nodes[j]\n",
    "                if t2 < t1:\n",
    "                    tmp = t1\n",
    "                    t1 = t2\n",
    "                    t2 = tmp\n",
    "                if (t1, t2) not in edge_dict:\n",
    "                    edge_dict[(t1, t2)] = ''\n",
    "                edge_dict[(t1, t2)] += '{},'.format(entry.icebreaker_id)\n",
    "    nx.set_edge_attributes(G, edge_dict, name='references')\n",
    "\n",
    "def assign_numeric_ids_to_nodes(node_set):\n",
    "    \"\"\"\n",
    "    Sometimes we just need numeric IDs, so here they are.\n",
    "    \"\"\"\n",
    "    label_to_numeric_id = {}\n",
    "    numeric_id_to_label = {}\n",
    "    for i, label in enumerate(node_set):\n",
    "        label_to_numeric_id[label] = i\n",
    "        numeric_id_to_label[i] = label\n",
    "    return label_to_numeric_id, numeric_id_to_label\n",
    "\n",
    "nodes = [x[0] for x in topic_counter.most_common()[20:50000] if x[0] != '']\n",
    "node_set = set(nodes)\n",
    "nodes_label_to_numeric_id, nodes_numeric_id_to_label = assign_numeric_ids_to_nodes(node_set)\n",
    "G = create_network(topic_counter, edge_dict, node_set)\n",
    "add_references_to_edges(get_entries(), G)\n",
    "G2 = reduce_network(G, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_graph(G, path):\n",
    "    \"\"\"\n",
    "    Writes the files node_list.csv and edge_list.csv.\n",
    "    \"\"\"\n",
    "    # Write node list\n",
    "    with open(os.path.join(path, 'node_list.csv'), 'wt') as f:\n",
    "        f.write('Name,Weight\\n')\n",
    "        for n, data in G.nodes(data=True):\n",
    "            w = data['weight']\n",
    "            line = '{},{}'.format(n, w)\n",
    "            f.write(line + '\\n')\n",
    "        \n",
    "    # Write edge list\n",
    "    with open(os.path.join(path, 'edge_list.csv'), 'wt') as f:\n",
    "        f.write('Source,Target,Type,Weight,NormalizedWeight\\n')\n",
    "        for n1, n2, data in G.edges(data=True):\n",
    "            line = '{},{},undefined_relation,{},{:.16f}'.format(n1, n2, data['weight'], data['normalized_weight'])\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "export_graph(G, os.path.join(private_data_path, 'size_50000/G'))\n",
    "export_graph(G2, os.path.join(private_data_path, 'size_50000/G2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def _node_name_to_neo4j_id(name):\n",
    "    return name.replace(' ', '_')\n",
    "\n",
    "\n",
    "def export_to_neo4j(G, path):\n",
    "    \"\"\"\n",
    "    Writes csv files that can be imported into neo4j. This file can be imported with the import tool\n",
    "    \"\"\"\n",
    "    # Write node list  \n",
    "    with open(os.path.join(path, 'nodes.csv'), 'wt') as f:\n",
    "        f.write(':ID,name,weight:int,:LABEL\\n')\n",
    "        for n, data in G.nodes(data=True):\n",
    "            w = data['weight']\n",
    "            line = '{},{},{},Topic'.format(_node_name_to_neo4j_id(n), n, w)\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    # Write edge list\n",
    "    with open(os.path.join(path, 'edges.csv'), 'wt') as f:\n",
    "        f.write(':START_ID,:END_ID,:TYPE,weight:int,normalizedWeight:float,references\\n')\n",
    "        csv_writer = csv.writer(f)\n",
    "        for n1, n2, data in G.edges(data=True):\n",
    "            csv_writer.writerow([_node_name_to_neo4j_id(n1), _node_name_to_neo4j_id(n2), 'RELATED_TO', \n",
    "                            data['weight'], data['normalized_weight'], data['references']])\n",
    "\n",
    "export_to_neo4j(G, os.path.join(private_data_path, 'size_50000/G_neo4j'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def export_top_nodes(G, k, path):\n",
    "    ns = sorted(list(G.nodes(data=True)), key=lambda n: n[1]['weight'], reverse=True)\n",
    "    ns = ns[:k]\n",
    "    ns = sorted(ns, key=lambda n: n[0])\n",
    "    with open(path, 'wt') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['Begriff', 'Anzahl Vorkommnisse'])\n",
    "        for n in ns:\n",
    "            label = n[0]\n",
    "            weight = n[1]['weight']\n",
    "            csv_writer.writerow([label, weight])\n",
    "\n",
    "export_top_nodes(G, 10000, os.path.join(private_data_path, 'begriffe.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 90}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes['2010']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "# https://github.com/eliorc/node2vec\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder='/tmp/node2vec')\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "with open(os.path.join(private_data_path, 'G_node2vec_model.pickle'), 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "model.wv.save_word2vec_format(os.path.join(private_data_path, 'G_node2vec.embd'))\n",
    "model.save(os.path.join(private_data_path, 'G_node2vec.model'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
